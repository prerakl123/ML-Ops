{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd763693-1c34-4044-a4e2-fff4c88b873d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\Prerak\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import nltk\n",
    "# nltk.download('movie_reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5595f822-6418-4bb2-a61a-1cd4135cc8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIVIDING TEXT USING CHUNKING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd14070c-b067-45e1-8c7d-e2630ce6dbdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of text chunks = 6\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from nltk.corpus import brown\n",
    "\n",
    "# Split a text into chunks\n",
    "def splitter(data, num_words):\n",
    "    words = data.split(' ')\n",
    "    output = []\n",
    "    \n",
    "    cur_count = 0\n",
    "    cur_words = []\n",
    "    for word in words:\n",
    "        cur_words.append(word)\n",
    "        cur_count += 1\n",
    "        if cur_count == num_words:\n",
    "            output.append(' '.join(cur_words))\n",
    "            cur_words = []\n",
    "            cur_count = 0\n",
    "    output.append(' '.join(cur_words) )\n",
    "    return output\n",
    "\n",
    "# Read the data from the Brown corpus\n",
    "data = ' '.join(brown.words()[:10000])\n",
    "# Number of words in each chunk\n",
    "num_words = 1700\n",
    "chunks = []\n",
    "counter = 0\n",
    "text_chunks = splitter(data, num_words)\n",
    "print(\"Number of text chunks =\", len(text_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "125f3c5e-a5ca-43b9-859d-ede2e146a8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUILDING A BAG-OF-WORDS MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c97e5d8-d49a-40f3-b640-e8e12ba28ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vocabulary:\n",
      "['about' 'after' 'against' 'aid' 'all' 'also' 'an' 'and' 'are' 'as' 'at'\n",
      " 'be' 'been' 'before' 'but' 'by' 'committee' 'congress' 'did' 'each'\n",
      " 'education' 'first' 'for' 'from' 'general' 'had' 'has' 'have' 'he'\n",
      " 'health' 'his' 'house' 'in' 'increase' 'is' 'it' 'last' 'made' 'make'\n",
      " 'may' 'more' 'no' 'not' 'of' 'on' 'one' 'only' 'or' 'other' 'out' 'over'\n",
      " 'pay' 'program' 'proposed' 'said' 'similar' 'state' 'such' 'take' 'than'\n",
      " 'that' 'the' 'them' 'there' 'they' 'this' 'time' 'to' 'two' 'under' 'up'\n",
      " 'was' 'were' 'what' 'which' 'who' 'will' 'with' 'would' 'year' 'years']\n",
      "\n",
      "Document term matrix:\n",
      "\n",
      "         Word     Chunk-0     Chunk-1     Chunk-2     Chunk-3     Chunk-4 \n",
      "\n",
      "       about           1           1           1           1           3\n",
      "       after           2           3           2           1           3\n",
      "     against           1           2           2           1           1\n",
      "         aid           1           1           1           3           5\n",
      "         all           2           2           5           2           1\n",
      "        also           3           3           3           4           3\n",
      "          an           5           7           5           7          10\n",
      "         and          34          27          36          36          41\n",
      "         are           5           3           6           3           2\n",
      "          as          13           4          14          18           4\n",
      "          at           5           7           9           3           6\n",
      "          be          20          14           7          10          18\n",
      "        been           7           1           6          15           5\n",
      "      before           2           2           1           1           2\n",
      "         but           3           3           2           9           5\n",
      "          by           8          22          15          14          12\n",
      "   committee           2          10           3           1           7\n",
      "    congress           1           1           3           3           1\n",
      "         did           2           1           1           2           2\n",
      "        each           1           1           4           3           1\n",
      "   education           3           2           3           1           1\n",
      "       first           4           1           4           6           3\n",
      "         for          22          19          24          27          20\n",
      "        from           4           5           6           5           5\n",
      "     general           2           2           2           3           6\n",
      "         had           3           2           7           2           6\n",
      "         has          10           2           5          20          11\n",
      "        have           4           4           4           7           5\n",
      "          he           4          13          12          13          29\n",
      "      health           1           1           2           6           1\n",
      "         his          10           6           9           3           7\n",
      "       house           5           7           4           4           2\n",
      "          in          38          27          37          49          45\n",
      "    increase           3           1           1           4           1\n",
      "          is          12           9          12          14           8\n",
      "          it          18          16           5           6           9\n",
      "        last           1           1           5           4           2\n",
      "        made           1           1           7           4           3\n",
      "        make           3           2           1           1           1\n",
      "         may           1           1           2           2           1\n",
      "        more           3           5           4           6           7\n",
      "          no           4           1           1           7           3\n",
      "         not           5           6           3          14           7\n",
      "          of          61          69          76          56          53\n",
      "          on          10          18          14          13          13\n",
      "         one           4           5           3           4           9\n",
      "        only           1           1           1           3           2\n",
      "          or           4           4           5           5           4\n",
      "       other           2           6           7           1           3\n",
      "         out           3           3           3           4           1\n",
      "        over           1           1           5           1           2\n",
      "         pay           2           3           5           4           1\n",
      "     program           2           1           4           4           5\n",
      "    proposed           2           2           1           1           1\n",
      "        said          20          15          11           9          21\n",
      "     similar           1           1           2           1           2\n",
      "       state          12           9           5           5           7\n",
      "        such           2           3           2           4           2\n",
      "        take           2           2           2           2           2\n",
      "        than           2           2           3           5           4\n",
      "        that          27          12          12          17          31\n",
      "         the         143         116         132         136         148\n",
      "        them           2           2           2           3           2\n",
      "       there           9           4           2           6           6\n",
      "        they           3           2           2           7           2\n",
      "        this           8           5           8           9           7\n",
      "        time           2           1           2           3          11\n",
      "          to          50          54          46          49          66\n",
      "         two           3           3           4           1           1\n",
      "       under           3           3           5           3           1\n",
      "          up           2           1           6           5           5\n",
      "         was          13          16          11           6          14\n",
      "        were           2           3           4           5           3\n",
      "        what           1           1           1           1           2\n",
      "       which          13          10           2           2           3\n",
      "         who           6           5           9           4           1\n",
      "        will          14           2           5          11           4\n",
      "        with           4           6           6           9          10\n",
      "       would           8          27          15           7          23\n",
      "        year           2           4           9          10           3\n",
      "       years           1           3           2           2           3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from nltk.corpus import brown\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Read the data from the Brown corpus\n",
    "data = ' '.join(brown.words()[:10000])\n",
    "\n",
    "# Number of words in each chunk\n",
    "num_words = 2000\n",
    "chunks = []\n",
    "counter = 0\n",
    "text_chunks = splitter(data, num_words)\n",
    "\n",
    "for text in text_chunks:\n",
    "    chunk = {'index': counter, 'text': text}\n",
    "    chunks.append(chunk)\n",
    "    counter += 1\n",
    "\n",
    "\n",
    "# Extract document term matrix\n",
    "vectorizer = CountVectorizer(min_df=5, max_df=.95)\n",
    "doc_term_matrix = vectorizer.fit_transform([chunk['text'] for chunk in chunks])\n",
    "vocab = np.array(vectorizer.get_feature_names_out())\n",
    "print(\"\\nVocabulary:\")\n",
    "print(vocab)\n",
    "\n",
    "print(\"\\nDocument term matrix:\")\n",
    "chunk_names = ['Chunk-0', 'Chunk-1', 'Chunk-2', 'Chunk-3', 'Chunk-4']\n",
    "formatted_row = '{:>12}' * (len(chunk_names) + 1)\n",
    "print('\\n', formatted_row.format('Word', *chunk_names), '\\n')\n",
    "\n",
    "for word, item in zip(vocab, doc_term_matrix.T):\n",
    "    # 'item' is a 'csr_matrix' data structure\n",
    "    output = [str(x) for x in item.data]\n",
    "    print(formatted_row.format(word, *output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b0b6fdb-ac97-44d6-85c2-39b6b8d3da02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUILDING A TEXT CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2a9740d-08c5-48d1-9e8e-9b8c4376da7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dimensions of training data: (2968, 40605)\n",
      "\n",
      "Input: The curveballs of right handed pitchers tend to curve to the left \n",
      "Predicted category: Baseball\n",
      "\n",
      "Input: Caesar cipher is an ancient form of encryption \n",
      "Predicted category: Cryptography\n",
      "\n",
      "Input: This two-wheeler is really good on slippery roads \n",
      "Predicted category: Motorcycles\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "category_map = {\n",
    "    'misc.forsale': 'Sales',\n",
    "    'rec.motorcycles': 'Motorcycles',\n",
    "    'rec.sport.baseball': 'Baseball',\n",
    "    'sci.crypt': 'Cryptography',\n",
    "    'sci.space': 'Space'\n",
    "}\n",
    "training_data = fetch_20newsgroups(subset='train', categories=category_map.keys(), shuffle=True, random_state=7)\n",
    "\n",
    "# Feature extraction\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_termcounts = vectorizer.fit_transform(training_data.data)\n",
    "print(\"\\nDimensions of training data:\", X_train_termcounts.shape)\n",
    "\n",
    "# Training a classifier\n",
    "input_data = [\n",
    "    \"The curveballs of right handed pitchers tend to curve to the left\",\n",
    "    \"Caesar cipher is an ancient form of encryption\",\n",
    "    \"This two-wheeler is really good on slippery roads\"\n",
    "]\n",
    "\n",
    "# tf-idf transformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_termcounts)\n",
    "\n",
    "# Multinomial Naive Bayes classifier\n",
    "classifier = MultinomialNB().fit(X_train_tfidf, training_data.target)\n",
    "\n",
    "X_input_termcounts = vectorizer.transform(input_data)\n",
    "X_input_tfidf = tfidf_transformer.transform(X_input_termcounts)\n",
    "\n",
    "# Predict the output categories\n",
    "predicted_categories = classifier.predict(X_input_tfidf)\n",
    "\n",
    "# Print the outputs\n",
    "for sentence, category in zip(input_data, predicted_categories):\n",
    "    print('\\nInput:', sentence, '\\nPredicted category:', category_map[training_data.target_names[category]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dcf66b56-18e8-4969-9dd5-e66b77a0e1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IDENTIFYING THE GENDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d133404-82df-4ff9-94f6-955289efee0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of letters: 1\n",
      "Accuracy ==> 76.2%\n",
      "Leonardo ==> male\n",
      "Amy ==> female\n",
      "Sam ==> male\n",
      "\n",
      "Number of letters: 2\n",
      "Accuracy ==> 78.60000000000001%\n",
      "Leonardo ==> male\n",
      "Amy ==> female\n",
      "Sam ==> male\n",
      "\n",
      "Number of letters: 3\n",
      "Accuracy ==> 76.6%\n",
      "Leonardo ==> male\n",
      "Amy ==> female\n",
      "Sam ==> female\n",
      "\n",
      "Number of letters: 4\n",
      "Accuracy ==> 70.8%\n",
      "Leonardo ==> male\n",
      "Amy ==> female\n",
      "Sam ==> female\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from nltk.corpus import names\n",
    "from nltk import NaiveBayesClassifier\n",
    "from nltk.classify import accuracy as nltk_accuracy\n",
    "\n",
    "# Extract features from the input word\n",
    "def gender_features(word, num_letters=2):\n",
    "    return {'feature': word[-num_letters:].lower()}\n",
    "\n",
    "\n",
    "# Extract labeled names\n",
    "labeled_names = ([(name, 'male') for name in names.words('male.txt')] + [(name, 'female') for name in names.words('female.txt')])\n",
    "random.seed(7)\n",
    "random.shuffle(labeled_names)\n",
    "input_names = ['Leonardo', 'Amy', 'Sam']\n",
    "\n",
    "\n",
    "# Sweeping the parameter space\n",
    "for i in range(1, 5):\n",
    "    print('\\nNumber of letters:', i)\n",
    "    featuresets = [(gender_features(n, i), gender) for (n, gender) in labeled_names]\n",
    "    train_set, test_set = featuresets[500:], featuresets[:500]\n",
    "    classifier = NaiveBayesClassifier.train(train_set)\n",
    "    \n",
    "    # Print classifier accuracy\n",
    "    print('Accuracy ==>', str(100 * nltk_accuracy(classifier, test_set)) + str('%'))\n",
    "\n",
    "    # Predict outputs for new inputs\n",
    "    for name in input_names:\n",
    "        print(name, '==>', classifier.classify(gender_features(name, i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5942dee-4471-40bb-8eb2-e816067fcf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANALYZING THE SENTIMENT OF A SENTENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ca90fa4-b51c-4339-b03f-944f17904ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of training datapoints: 1600\n",
      "Number of test datapoints: 400\n",
      "\n",
      "Accuracy of the classifier: 0.735\n",
      "\n",
      "Top 10 most informative words:\n",
      "outstanding\n",
      "insulting\n",
      "vulnerable\n",
      "ludicrous\n",
      "uninvolving\n",
      "astounding\n",
      "avoids\n",
      "fascination\n",
      "affecting\n",
      "animators\n",
      "\n",
      "Predictions:\n",
      "\n",
      "Review: It is an amazing movie\n",
      "Predicted sentiment: Positive\n",
      "Probability: 0.61\n",
      "\n",
      "Review: This is a dull movie. I would never recommend it to anyone.\n",
      "Predicted sentiment: Negative\n",
      "Probability: 0.77\n",
      "\n",
      "Review: The cinematography is pretty great in this movie\n",
      "Predicted sentiment: Positive\n",
      "Probability: 0.67\n",
      "\n",
      "Review: The direction was terrible and the story was all over the place\n",
      "Predicted sentiment: Negative\n",
      "Probability: 0.63\n"
     ]
    }
   ],
   "source": [
    "import nltk.classify.util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "def extract_features(word_list):\n",
    "    return dict([(word, True) for word in word_list])\n",
    "\n",
    "\n",
    "# Load positive and negative reviews\n",
    "positive_fileids = movie_reviews.fileids('pos')\n",
    "negative_fileids = movie_reviews.fileids('neg')\n",
    "\n",
    "features_positive = [(extract_features(movie_reviews.words(fileids=[f])), 'Positive') for f in positive_fileids]\n",
    "features_negative = [(extract_features(movie_reviews.words(fileids=[f])), 'Negative') for f in negative_fileids]\n",
    "\n",
    "# Split the data into train and test (80/20)\n",
    "threshold_factor = 0.8\n",
    "threshold_positive = int(threshold_factor * len(features_positive))\n",
    "threshold_negative = int(threshold_factor * len(features_negative))\n",
    "\n",
    "features_train = features_positive[:threshold_positive] + features_negative[:threshold_negative]\n",
    "features_test = features_positive[threshold_positive:] + features_negative[threshold_negative:]\n",
    "print(\"\\nNumber of training datapoints:\", len(features_train))\n",
    "print(\"Number of test datapoints:\", len(features_test))\n",
    "\n",
    "# Train a Naive Bayes classifier\n",
    "classifier = NaiveBayesClassifier.train(features_train)\n",
    "print(\"\\nAccuracy of the classifier:\", nltk.classify.util.accuracy(classifier, features_test))\n",
    "print(\"\\nTop 10 most informative words:\")\n",
    "for item in classifier.most_informative_features()[:10]:\n",
    "    print(item[0])\n",
    "\n",
    "# Sample input reviews\n",
    "input_reviews = [\n",
    "    \"It is an amazing movie\",\n",
    "    \"This is a dull movie. I would never recommend it to anyone.\",\n",
    "    \"The cinematography is pretty great in this movie\",\n",
    "    \"The direction was terrible and the story was all over the place\"\n",
    "]\n",
    "\n",
    "print(\"\\nPredictions:\")\n",
    "for review in input_reviews:\n",
    "    print(\"\\nReview:\", review)\n",
    "    probdist = classifier.prob_classify(extract_features(review.split()))\n",
    "    \n",
    "    pred_sentiment = probdist.max()\n",
    "    print(\"Predicted sentiment:\", pred_sentiment)\n",
    "    print(\"Probability:\", round(probdist.prob(pred_sentiment), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86312105-3f9b-4e44-a3b0-9db38acfd7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IDENTIFYING ERRORS IN TEXT USING TOPIC MODELLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea02b608-837c-4506-a557-7f6716d4aa6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from gensim import models, corpora\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "# Load input data\n",
    "def load_data(input_file):\n",
    "    data = []\n",
    "    with open(input_file, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            data.append(line[:-1])\n",
    "\n",
    "    return data\n",
    "\n",
    "class Preprocessor(object):\n",
    "    \n",
    "    # Initialize various operators\n",
    "    def __init__(self):\n",
    "        \n",
    "        # Create a regular expression tokenizer\n",
    "        self.tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        \n",
    "        # get the list of stop words\n",
    "        self.stop_words_english = stopwords.words('english')\n",
    "\n",
    "        # Create a Snowball stemmer\n",
    "        self.stemmer = SnowballStemmer('english')\n",
    "\n",
    "    # Tokenizing, stop word removal, and stemming\n",
    "    def process(self, input_text):\n",
    "        \n",
    "        # Tokenize the string\n",
    "        tokens = self.tokenizer.tokenize(input_text.lower())\n",
    "        \n",
    "        # Remove the stop words\n",
    "        tokens_stopwords = [x for x in tokens if not x in self.stop_words_english]\n",
    "\n",
    "        # Perform stemming on the tokens\n",
    "        tokens_stemmed = [self.stemmer.stem(x) for x in tokens_stopwords]\n",
    "\n",
    "        return tokens_stemmed\n",
    "\n",
    "\n",
    "# File containing linewise input data\n",
    "input_file = 'data_topic_modeling.txt'\n",
    "\n",
    "# Load data\n",
    "data = load_data(input_file)\n",
    "\n",
    "# Create a preprocessor object\n",
    "preprocessor = Preprocessor()\n",
    "\n",
    "# Create a list for processed documents\n",
    "processed_tokens = [preprocessor.process(x) for x in data]\n",
    "\n",
    "# Create a dictionary based on the tokenized documents\n",
    "dict_tokens = corpora.Dictionary(processed_tokens)\n",
    "\n",
    "# Create a document-term matrix\n",
    "corpus = [dict_tokens.doc2bow(text) for text in processed_tokens]\n",
    "\n",
    "# Generate the LDA model based on the corpus we just created\n",
    "num_topics = 2\n",
    "num_words = 4\n",
    "ldamodel = models.ldamodel.LdaModel(corpus, num_topics=num_topics, id2word=dict_tokens, passes=25)\n",
    "\n",
    "print(\"Most contributing words to the topics:\")\n",
    "for item in ldamodel.print_topics(num_topics=num_topics, num_words=num_words):\n",
    "    print(\"\\nTopic\", item[0], \"==>\", item[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
